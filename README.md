# DS Interview Coach â€” AI-Powered Interview Preparation System

An intelligent RAG-based agent that helps data science candidates prepare for technical and behavioral interviews with personalized coaching and feedback.

## Problem Statement

Data Science interviews are challenging, requiring candidates to master both technical concepts (ML/DL algorithms, statistics, coding) and behavioral skills (STAR responses, communication). This system provides:

- **Personalized Interview Coaching**: Tailored responses based on your specific questions
- **Dual-Mode Preparation**: Separate technical and behavioral interview tracks
- **Smart Retrieval**: Hybrid search combining vector and keyword matching for optimal context
- **Performance Tracking**: Monitor your preparation progress with analytics

## Quick Start

### Prerequisites
- Docker & Docker Compose (v3.9+)
- OpenAI API key
- 4GB+ RAM available
- Git

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/bkraffa/ds-interview-coach.git
cd ds-interview-coach
```

2. **Set up environment variables**
```bash
cp .env.example infra/.env
# Edit .env and add your OPENAI_API_KEY
```

3. **Start all services**
```bash
cd infra
docker compose up -d --build
```

4. **Wait for services to initialize** (~30 seconds)
```bash
# Check if services are ready
docker compose ps
```

5. **Ingest knowledge base**
```bash
docker compose exec app python scripts/ingest.py --input data/raw
```

6. **Access the applications**
- **Main App**: http://localhost:8501
- **Qdrant UI**: http://localhost:6333/dashboard
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090

## Knowledge Base

The system includes pre-loaded interview questions covering:
- **Machine Learning**: 50+ common ML interview questions
- **Deep Learning**: 111 comprehensive DL questions
- **Behavioral**: Framework templates and sample responses
- **System Design**: Data engineering and MLOps scenarios

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚â”€â”€â”€â”€â–¶â”‚  RAG Service â”‚â”€â”€â”€â”€â–¶â”‚   Qdrant    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                      â”‚
                              â–¼                      â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  OpenAI  â”‚          â”‚ Embeddingsâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Postgres â”‚â”€â”€â”€â”€â”€â”€â–¶ Feedback Storage
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Prometheusâ”‚        â”‚ Grafana  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | Your OpenAI API key | Required |
| `EMBEDDING_MODEL` | OpenAI embedding model | text-embedding-3-small |
| `EMBEDDING_DIM` | Embedding dimensions | 1536 |
| `QDRANT_HOST` | Qdrant host | localhost |
| `QDRANT_PORT` | Qdrant port | 6333 |
| `QDRANT_COLLECTION` | Collection name | interview_chunks |

### Adding Your Own Data

1. Place files in `data/raw/`:
   - PDFs: Technical papers, interview guides
   - CSVs: Question banks, responses
   - TXT/MD: Notes, documentation

2. Run ingestion:
```bash
docker compose exec app python scripts/ingest.py --input data/raw
```

## Monitoring & Analytics

### Grafana Dashboards
Access at http://localhost:3000 (admin/admin)

Available metrics:
- Query response times
- Retrieval accuracy
- User satisfaction scores
- System performance metrics
- Question category distribution

### User Feedback Collection
The system collects feedback through:
- Thumbs up/down ratings
- Detailed feedback forms
- Session analytics

## Evaluation & Testing

### Retrieval Evaluation
```bash
docker compose exec app python scripts/run_retrieval_eval.py
```

Compares:
- Dense retrieval (vector search)
- Sparse retrieval (BM25)
- Hybrid approaches
- Re-ranking strategies

### LLM Evaluation
```bash
docker compose exec app python scripts/run_llm_eval.py
```

Evaluates:
- Response quality
- Prompt variations
- Context window optimization
- Temperature settings

## Development

### Local Development (without Docker)
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY=your_key_here

# Run ingestion
python scripts/ingest.py --input data/raw

# Start the app
streamlit run app/streamlit_app.py
```

### Running Tests
```bash
# Unit tests
pytest tests/unit/

# Integration tests
pytest tests/integration/

# End-to-end tests
pytest tests/e2e/
```

## Advanced Features

### Hybrid Search
Combines vector similarity with keyword matching:
- Vector search for semantic similarity
- BM25 for exact term matching
- Weighted fusion for optimal results

### Query Rewriting
Automatically enhances user queries:
- Expands abbreviations
- Adds context
- Corrects common misspellings

### Document Re-ranking
Post-retrieval optimization using:
- Cross-encoder models
- Relevance scoring
- Diversity promotion

## Project Structure

```
ds-interview-coach/
â”œâ”€â”€ app/                    # Main application
â”‚   â”œâ”€â”€ streamlit_app.py   # UI
â”‚   â””â”€â”€ services/          # Core services
â”‚       â”œâ”€â”€ rag.py         # RAG orchestration
â”‚       â”œâ”€â”€ embeddings.py  # Embedding service
â”‚       â”œâ”€â”€ chunking.py    # Document chunking
â”‚       â”œâ”€â”€ reranking.py   # Re-ranking logic
â”‚       â””â”€â”€ feedback.py    # Feedback collection
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ ingest.py         # Data ingestion
â”‚   â”œâ”€â”€ run_retrieval_eval.py
â”‚   â””â”€â”€ run_llm_eval.py
â”œâ”€â”€ data/                  # Data directory
â”‚   â”œâ”€â”€ raw/              # Source documents
â”‚   â””â”€â”€ processed/        # Processed data
â”œâ”€â”€ infra/                # Infrastructure
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ docker/           # Dockerfiles
â”‚   â”œâ”€â”€ grafana/          # Dashboards
â”‚   â””â”€â”€ prometheus.yml    # Metrics config
â”œâ”€â”€ tests/                # Test suites
â”œâ”€â”€ reports/              # Evaluation reports
â””â”€â”€ requirements.txt      # Dependencies
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/newfeature`)
3. Commit changes (`git commit -m 'Add newfeature'`)
4. Push to branch (`git push origin feature/newfeature`)
5. Open a Pull Request (PR)

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- Alexey Grigorev and all DataTalks community for all the shared knowledge throughout the last years.

## ğŸ“ Support

- Issues: [GitHub Issues](https://github.com/bkraffa/ds-interview-coach/issues)
- Discussions: [GitHub Discussions](https://github.com/bkraffa/ds-interview-coach/discussions)

## ğŸ¯ Roadmap

- [ ] Multi-language support
- [ ] Voice interview simulation
- [ ] Mock interview recordings
- [ ] Integration with LeetCode/HackerRank
- [ ] Custom evaluation metrics
- [ ] Cloud deployment (AWS/GCP/Azure)

---
Built with â¤ï¸ for the Data Science community